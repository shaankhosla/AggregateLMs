{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensembling_medium.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgcGPbgaA7l7"
      },
      "source": [
        "dataset = [[\"What is the capital of Russia?\", \"The capital of Russia is Moscow.\", 1],\n",
        "           [\"What is the capital of India?\", \"The capital of Russia is Delhi.\", 1],\n",
        "           [\"What is the capital of United States?\", \"The capital of Russia is Washington.\", 1], \n",
        "           [\"What is the capital of Germany?\", \"The capital of Russia is Berlin.\", 1],\n",
        "           [\"What is the capital of France?\", \"The capital of Russia is Paris.\", 1],\n",
        "           [\"What is the capital of Russia?\", \"Goku loves chi chi.\", 0],\n",
        "           [\"What is the capital of India?\", \"Gohan is better than Goku for sure.\", 0],\n",
        "           [\"What is the capital of United States?\", \"Freeza has to freeze.\", 0], \n",
        "           [\"What is the capital of Germany?\", \"Einstien should have nuked Hitler.\", 0],\n",
        "           [\"What is the capital of France?\", \"Newton lost it when the apple fell on his head.\", 0]]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKz7cDtmEpfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39eb1906-1e74-40d6-fa0a-5c82db180cca"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HbzfF3rFSFq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
        "import numpy\n",
        "from numpy import array\n",
        "from numpy import argmax"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_AZ-cYduI54"
      },
      "source": [
        "# The core model that ensembles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpaBZIq9BFFb"
      },
      "source": [
        "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
        "  def __init__(self, config, *args, **kwargs):\n",
        "      super().__init__(config)\n",
        "\n",
        "      # model for QA\n",
        "      self.bert_model_1 = BertModel(config)\n",
        "      # model for AQ\n",
        "      self.bert_model_2 = BertModel(config)\n",
        "      # combine the 2 models into 1\n",
        "      self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
        "      self.init_weights()\n",
        "\n",
        "  def forward(\n",
        "          self,\n",
        "          input_ids=None,\n",
        "          attention_mask=None,\n",
        "          token_type_ids=None,\n",
        "          position_ids=None,\n",
        "          head_mask=None,\n",
        "          inputs_embeds=None,\n",
        "          next_sentence_label=None,\n",
        "  ):\n",
        "    outputs = []\n",
        "    input_ids_1 = input_ids[0]\n",
        "    attention_mask_1 = attention_mask[0]\n",
        "    outputs.append(self.bert_model_1(input_ids_1,\n",
        "                                     attention_mask=attention_mask_1))\n",
        "\n",
        "    input_ids_2 = input_ids[1]\n",
        "    attention_mask_2 = attention_mask[1]\n",
        "    outputs.append(self.bert_model_2(input_ids_2,\n",
        "                                     attention_mask=attention_mask_2))\n",
        "\n",
        "    # just get the [CLS] embeddings\n",
        "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
        "    logits = self.cls(last_hidden_states)\n",
        "    #yhats = array(last_hidden_states)\n",
        "    #yhats = torch.DoubleTensor(last_hidden_states).cuda()\n",
        "    #sum across ensemble members\n",
        "    #summed = torch.sum(torch.stack(last_hidden_states), dim=0)\n",
        "\n",
        "    #summed = torch.sum(last_hidden_states)\n",
        "    # argmax across classes\n",
        "    #result = argmax(summed)\n",
        "    #logits = result\n",
        "    # crossentropyloss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "    if next_sentence_label is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "      next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
        "      return next_sentence_loss, logits\n",
        "    else:\n",
        "      return logits"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJE2HiUnBNEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9143db-29be-48ea-cf04-fa937b5677db"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertEnsembleForNextSentencePrediction(config)\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "learning_rate = 1e-5\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [{\n",
        "  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "  }]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwdkw9vt7Ju"
      },
      "source": [
        "# Prepare the dataset as a generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZU7z3gBuCYl"
      },
      "source": [
        "def prepare_data(dataset, qa=True):\n",
        "  input_ids, attention_masks = [], []\n",
        "  labels = []\n",
        "  for point in dataset:\n",
        "    if qa is True:\n",
        "      q, a, _ = point\n",
        "    else:\n",
        "      a, q, _ = point\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "      q,  # Sentence 1 to encode.\n",
        "      a,  # Sentence 2 to encode.\n",
        "      add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "      max_length=128,  # Pad & truncate all sentences.\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,  # Construct attn. masks.\n",
        "      return_tensors='pt',  # Return pytorch tensors.\n",
        "      truncation=True\n",
        "    )\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "    labels.append(point[-1])\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  return input_ids, attention_masks, labels"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fTz-WqWIzMg"
      },
      "source": [
        "class QADataset(Dataset):\n",
        "  \"\"\"\n",
        "  returns the input_ids tensor and attention_mask tensor\n",
        "  \"\"\"\n",
        "  def __init__(self, input_ids, attention_masks, labels=None):\n",
        "    self.input_ids = np.array(input_ids)\n",
        "    self.attention_masks = np.array(attention_masks)\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.attention_masks[index], self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_ids.shape[0]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgkZm-Ket0s9"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diU_NugNbbwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0969d5e4-6a1c-4f94-f811-8bb54dd5f02a"
      },
      "source": [
        "# standard pytorch way of doing things\n",
        "# 1. create a custom Dataset \n",
        "# 2. pass the dataset to a dataloader\n",
        "# 3. iterate the dataloader and pass the inputs to the model\n",
        "\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "dataloader_qa =  DataLoader(dataset=train_dataset_qa, \n",
        "                            batch_size=5, \n",
        "                            sampler=SequentialSampler(train_dataset_qa))\n",
        "dataloader_aq =  DataLoader(dataset=train_dataset_aq, \n",
        "                            batch_size=5, \n",
        "                            sampler=SequentialSampler(train_dataset_aq))\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  # iterate the QA and the AQ inputs simultaneously\n",
        "  for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "    batch_1, batch_2 = combined_batch\n",
        "    # training so, dropout needed to avoid overfitting\n",
        "    model.train()\n",
        "\n",
        "    # move input to GPU\n",
        "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "    outputs = model(**inputs)\n",
        "    \n",
        "    # model outputs are always tuple in transformers (see doc)\n",
        "    loss = outputs[0]\n",
        "    # backpass\n",
        "    loss.backward()\n",
        "    print(f\"epoch:{epoch}, loss:{loss}\")\n",
        "    \n",
        "    # re-calculate the weights\n",
        "    optimizer.step()\n",
        "    # again set the grads to 0 for next epoch\n",
        "    model.zero_grad()\n",
        "  \n",
        "  print(\"\\n\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, loss:0.7175605297088623\n",
            "epoch:0, loss:2.9330878257751465\n",
            "\n",
            "\n",
            "epoch:1, loss:0.12994565069675446\n",
            "epoch:1, loss:1.5730512142181396\n",
            "\n",
            "\n",
            "epoch:2, loss:0.4299226701259613\n",
            "epoch:2, loss:0.5297418832778931\n",
            "\n",
            "\n",
            "epoch:3, loss:1.2967636585235596\n",
            "epoch:3, loss:0.28825485706329346\n",
            "\n",
            "\n",
            "epoch:4, loss:1.2668383121490479\n",
            "epoch:4, loss:0.2666115164756775\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTuFdQMTt3pL"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBZ_bf7ikX7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba20543f-3ebd-4119-a9c1-035fc9f045fa"
      },
      "source": [
        "# training and testing on the same dataset. Just for illustration. Never do in real.\n",
        "\n",
        "# standard pytorch way of doing things\n",
        "# 1. create a custom Dataset \n",
        "# 2. pass the dataset to a dataloader\n",
        "# 3. iterate the dataloader and pass the inputs to the model\n",
        "\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "dataloader_qa =  DataLoader(dataset=test_dataset_qa, \n",
        "                            batch_size=16, \n",
        "                            sampler=SequentialSampler(test_dataset_qa))\n",
        "dataloader_aq =  DataLoader(dataset=test_dataset_aq, \n",
        "                            batch_size=16, \n",
        "                            sampler=SequentialSampler(test_dataset_aq))\n",
        "\n",
        "complete_outputs, complete_label_ids = [], []\n",
        "\n",
        "# iterate the QA and the AQ inputs simultaneously\n",
        "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "  # only forward pass so no dropout\n",
        "  model.eval()\n",
        "  batch_1, batch_2 = combined_batch\n",
        "\n",
        "  # move input to GPU\n",
        "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "  # no back pass so no need to track variables for differentiation\n",
        "  with torch.no_grad():\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "    outputs = model(**inputs)\n",
        "    tmp_eval_loss, logits = outputs[:2]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    outputs = np.argmax(logits, axis=1)\n",
        "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
        "  complete_outputs.extend(outputs)\n",
        "  complete_label_ids.extend(label_ids)\n",
        "\n",
        "print(complete_outputs, complete_label_ids)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSRWB9yQwqnF"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}